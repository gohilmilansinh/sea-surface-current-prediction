# -*- coding: utf-8 -*-
"""Final_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H9YH0HuK8vzPwetKkzabQvKI5OWikm9X
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error
import xarray as xr

# Load the NetCDF file
file_path = 'adt.nc'
data = xr.open_dataset(file_path)

# Inspect the contents
data

adt_data = data.adt.values

adt_data.shape

adt_tensor = torch.from_numpy(adt_data)

adt_tensor = adt_tensor.reshape((1254,28800))
adt_tensor.shape

adt_tensor = torch.transpose(adt_tensor, 1,0)
adt_tensor.shape

adt_tensor.nan_to_num_(adt_tensor.nanmean())

adt_tensor.isnan().sum()

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from torch import nn
from torch.utils.data import DataLoader, Dataset,TensorDataset

scaler = MinMaxScaler(feature_range=(0,1))
dataset = scaler.fit_transform(adt_tensor)

dataset

train_size = int(dataset.shape[1]*0.7)
test_size = int(dataset.shape[1]*0.2)
evalute_size = dataset.shape[1]-train_size-test_size
train_size,test_size,evalute_size

train,test,evaluate = dataset[:,0:train_size], dataset[:,train_size:test_size+train_size], dataset[:,test_size+train_size:len(dataset)]

train.shape,test.shape,evaluate.shape

def to_sequences(dataset,seq_size=1):
    x = []
    y = []
    for i in range(dataset.shape[1]-seq_size):
        window = dataset[:,i:(i+seq_size)]
        x.append(window)
        y.append(dataset[:,i+seq_size])
    x,y = np.array(x), np.array(y)
    return torch.tensor(x,dtype=torch.float32), torch.tensor(y,dtype=torch.float32)

seq_size = 10

trainX, trainy = to_sequences(train,seq_size)
testX, testy = to_sequences(test,seq_size)
evaluateX, evaluatey = to_sequences(evaluate,seq_size)

trainy = trainy.unsqueeze(-1)
testy = testy.unsqueeze(-1)
evaluatey = evaluatey.unsqueeze(-1)

print(trainX.shape, trainy.shape)
print(testX.shape, testy.shape)

class CustomDataset(Dataset):
    def __init__(self,x_tensor,y_tensor):
        self.xdomain = x_tensor
        self.ydomain = y_tensor
    def __len__(self):
        return len(self.xdomain)
    def __getitem__(self,index):
        return (self.xdomain[index],self.ydomain[index])

train_dataset = CustomDataset(trainX,trainy)
test_dataset = CustomDataset(testX,testy)
evaluate_dataset = CustomDataset(evaluateX,evaluatey)

train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False, pin_memory=True)
test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)
evaluate_dataloader = DataLoader(evaluate_dataset, batch_size=1, shuffle=False)

class ADTForecaster(nn.Module):
    def __init__(self, input_size=10, hidden_size=64, num_layers=1):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.2
        )
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 28800)  # Directly output all locations
        )

    def forward(self, x):
        out, _ = self.lstm(x)  # (batch, seq_len, hidden_size)
        out = self.fc(out[:, -1, :])  # Use last timestep's output
        return out.unsqueeze(-1)  # Add output dimension (batch, 28800, 1)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = ADTForecaster().to(device)

criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)

accumulation_steps = 8

# Training Loop
def train_model(model, train_loader, val_loader, epochs=50):
    best_val_loss = float('inf')
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        optimizer.zero_grad()

        for i, (batch_x, batch_y) in enumerate(train_loader):
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)

            outputs = model(batch_x)
            loss = criterion(outputs, batch_y) / accumulation_steps
            loss.backward()

            if (i + 1) % (accumulation_steps *10) == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                optimizer.zero_grad()

            train_loss += loss.item() * batch_x.size(0)

        # Validation
        val_loss = 0
        model.eval()
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                outputs = model(batch_x)
                val_loss += criterion(outputs, batch_y).item() * batch_x.size(0)

        # Update learning rate
        scheduler.step(val_loss)

        # Print progress
        train_loss = train_loss / len(train_loader.dataset)
        val_loss = val_loss / len(val_loader.dataset)
        print(f'Epoch {epoch+1:02} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')

        # Early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'best_model.pth')
            patience = 0
        else:
            patience += 1
            if patience >= 5:
                print("Early stopping")
                break
train_model(model, train_dataloader, test_dataloader, epochs=50)

"""# Evalutation with 10%"""

def evaluate_model(model, test_loader, scaler):
    model.eval()
    predictions = []
    actuals = []

    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)

            outputs = outputs.squeeze(-1).cpu().numpy()
            batch_y = batch_y.squeeze(-1).cpu().numpy()

            outputs = outputs.T
            batch_y = batch_y.T

            outputs_denorm = scaler.inverse_transform(outputs[:1254, :].T)
            targets_denorm = scaler.inverse_transform(batch_y[:1254, :].T)

            predictions.append(outputs_denorm)
            actuals.append(targets_denorm)

    predictions = np.vstack(predictions)
    actuals = np.vstack(actuals)

    rmse = np.sqrt(mean_squared_error(actuals, predictions))
    mae = mean_absolute_error(actuals, predictions)

    print(f'Test RMSE: {rmse:.4f}')
    print(f'Test MAE: {mae:.4f}')

evaluate_model(model, evaluate_dataloader, scaler)

def evaluate_accuracy(model, test_loader, scaler):
    model.eval()
    predictions = []
    actuals = []

    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)

            outputs = outputs.squeeze(-1).cpu().numpy()
            batch_y = batch_y.squeeze(-1).cpu().numpy()

            outputs_fixed = outputs[:, :1254]
            batch_y_fixed = batch_y[:, :1254]

            outputs_denorm = scaler.inverse_transform(outputs_fixed)
            targets_denorm = scaler.inverse_transform(batch_y_fixed)

            predictions.append(outputs_denorm)
            actuals.append(targets_denorm)

    predictions = np.vstack(predictions)
    actuals = np.vstack(actuals)

    mae = mean_absolute_error(actuals, predictions)

    mean_actual = np.mean(np.abs(actuals))

    # Compute Accuracy
    accuracy = max(0, 100 - (mae / mean_actual * 100))

    print(f"Test Accuracy: {accuracy:.2f}%")

evaluate_accuracy(model, evaluate_dataloader, scaler)

"""Testing data Evaluation"""

def evaluate_model(model, test_loader, scaler):
    model.eval()
    predictions = []
    actuals = []

    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)

            outputs = outputs.squeeze(-1).cpu().numpy()
            batch_y = batch_y.squeeze(-1).cpu().numpy()

            outputs = outputs.T
            batch_y = batch_y.T

            outputs_denorm = scaler.inverse_transform(outputs[:1254, :].T)
            targets_denorm = scaler.inverse_transform(batch_y[:1254, :].T)

            predictions.append(outputs_denorm)
            actuals.append(targets_denorm)

    predictions = np.vstack(predictions)
    actuals = np.vstack(actuals)

    # Compute evaluation metrics
    rmse = np.sqrt(mean_squared_error(actuals, predictions))
    mae = mean_absolute_error(actuals, predictions)

    print(f'Test RMSE: {rmse:.4f}')
    print(f'Test MAE: {mae:.4f}')

evaluate_model(model, test_dataloader, scaler)

def evaluate_accuracy(model, test_loader, scaler):
    model.eval()
    predictions = []
    actuals = []

    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)

            outputs = outputs.squeeze(-1).cpu().numpy()
            batch_y = batch_y.squeeze(-1).cpu().numpy()

            outputs_fixed = outputs[:, :1254]
            batch_y_fixed = batch_y[:, :1254]

            outputs_denorm = scaler.inverse_transform(outputs_fixed)
            targets_denorm = scaler.inverse_transform(batch_y_fixed)

            predictions.append(outputs_denorm)
            actuals.append(targets_denorm)

    predictions = np.vstack(predictions)
    actuals = np.vstack(actuals)

    mae = mean_absolute_error(actuals, predictions)

    mean_actual = np.mean(np.abs(actuals))

    accuracy = max(0, 100 - (mae / mean_actual * 100))

    print(f"Test Accuracy: {accuracy:.2f}%")

evaluate_accuracy(model, test_dataloader, scaler)

"""Checking for overfitting"""

def evaluate_accuracy(model, test_loader, scaler):
    model.eval()
    predictions = []
    actuals = []

    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)

            outputs = outputs.squeeze(-1).cpu().numpy()
            batch_y = batch_y.squeeze(-1).cpu().numpy()

            outputs_fixed = outputs[:, :1254]
            batch_y_fixed = batch_y[:, :1254]

            outputs_denorm = scaler.inverse_transform(outputs_fixed)
            targets_denorm = scaler.inverse_transform(batch_y_fixed)

            predictions.append(outputs_denorm)
            actuals.append(targets_denorm)

    predictions = np.vstack(predictions)
    actuals = np.vstack(actuals)

    mae = mean_absolute_error(actuals, predictions)

    mean_actual = np.mean(np.abs(actuals))

    # Compute Accuracy
    accuracy = max(0, 100 - (mae / mean_actual * 100))

    print(f"Test Accuracy: {accuracy:.2f}%")

evaluate_accuracy(model, train_dataloader, scaler)
